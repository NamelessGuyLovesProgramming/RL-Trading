# RL Trading System mit Human-in-the-Loop Feedback

Ein interaktives Reinforcement Learning Trading System mit **modularisierter Streamlit Trading App**, inspiriert vom Trackmania Beispiel, wo gezieltes Human Feedback verwendet wird, um der KI spezifische Trading-Patterns beizubringen.

## ğŸš€ Zwei Modi verfÃ¼gbar:

### 1. **Streamlit Trading App** (Neu - Modularisiert)
```bash
# Neue modularisierte Trading App starten
py -m streamlit run src/app.py --server.port 8504 --server.headless true
```
- TradingView Lightweight Charts
- NQ=F Standard-Asset, UTC+2 Zeitzone
- Debug-Modus mit historischer Simulation
- Modularisierte, erweiterbare Architektur

### 2. **RL Training System** (Original)
```bash
# RL Training mit Human Feedback
python src/main.py --mode demo --episodes 3
```
- Human-in-the-Loop Training
- Pattern Detection (FVG, Order Blocks)
- Live Data Integration

## ğŸ¯ Kern-Features

### ğŸ¤– Modulares Reward System
- **PnL Rewards**: Standard Trading Performance
- **FVG Rewards**: Fair Value Gap Bonuses (wie Trackmania Drift-Punkte)
- **Order Block Rewards**: Support/Resistance Trading
- **Liquidity Zone Rewards**: Magnet-Zonen fÃ¼r Preisbewegungen
- **Human Feedback**: Manuelles Training durch Bewertungen
- **Risk Management**: Bestrafung fÃ¼r schlechtes Risk Management

### ğŸ“Š Pattern Detection
- **Fair Value Gaps (FVG)**: PreislÃ¼cken die als Magnete wirken
- **Order Blocks**: High-Volume Support/Resistance Zonen
- **Liquidity Zones**: Bereiche mit hoher LiquiditÃ¤t
- **Market Structure**: Trend-Erkennung (Higher Highs/Lower Lows)

### ğŸ”´ Live Data Integration
- **Binance WebSocket**: Real-time Kerzendaten
- **Historical Data**: REST API fÃ¼r Backtesting
- **Multi-Timeframe**: 1m, 5m, 15m, 1h Support
- **Error Handling**: Automatische Reconnection

### ğŸ® Human-in-the-Loop Training
- **Demo Mode**: Menschliche Demonstrationen sammeln
- **Interactive Training**: Periodisches Feedback wÃ¤hrend Training
- **Pattern Learning**: KI lernt von deinen Trading-Entscheidungen
- **Adaptive Learning**: Learning Rate basierend auf Performance

## ğŸš€ Quick Start

### Installation

```bash
# Clone Repository
git clone <repository-url>
cd RL-Trading

# Install Dependencies
pip install -r requirements.txt

# Setup Environment
cp .env.template .env
# Edit .env mit deinen Binance API Keys (optional)
```

### Demo Mode (Empfohlen fÃ¼r Anfang)

```bash
# Sammle menschliche Demonstrationen
python src/main.py --mode demo --episodes 3

# Mit spezifischem Symbol
python src/main.py --mode demo --symbol ETHUSDT --episodes 5
```

In diesem Modus tradest du manuell und bewertest deine eigenen Trades. Das System lernt von deinen Entscheidungen.

### Training Mode

```bash
# Training mit Human Feedback
python src/main.py --mode train --timesteps 20000

# Mit Live-Daten (API Keys erforderlich)
python src/main.py --mode train --live --timesteps 10000
```

WÃ¤hrend des Trainings wirst du periodisch nach Feedback fÃ¼r die Performance des Agents gefragt.

### Evaluation Mode

```bash
# Evaluiere trainierten Agent
python src/main.py --mode eval --episodes 10

# Mit spezifischem Modell
python src/main.py --mode eval --model models/trading_agent_20231201_143022 --episodes 5
```

## ğŸ“ Projektstruktur

```
RL-Trading/
â”œâ”€â”€ .claude/                    # Claude Code Settings
â”œâ”€â”€ .claude-preferences.md      # Entwickler Preferences & Standards
â”œâ”€â”€ src/                        # Hauptquellcode (modularisiert)
â”‚   â”œâ”€â”€ app.py                  # Streamlit Trading App (NEU)
â”‚   â”œâ”€â”€ main.py                 # RL Training Entry Point
â”‚   â”œâ”€â”€ config/                 # Konfigurationsdateien
â”‚   â”‚   â””â”€â”€ settings.py         # App-weite Einstellungen
â”‚   â”œâ”€â”€ components/             # UI Components (NEU)
â”‚   â”‚   â”œâ”€â”€ chart.py           # TradingView Chart Komponente
â”‚   â”‚   â”œâ”€â”€ sidebar.py         # Sidebar mit Einstellungen
â”‚   â”‚   â””â”€â”€ trading_panel.py   # Trading Panel & Controls
â”‚   â”œâ”€â”€ data/                   # Datenverarbeitung
â”‚   â”‚   â””â”€â”€ yahoo_finance.py   # Yahoo Finance API Integration
â”‚   â”œâ”€â”€ utils/                  # Hilfsfunktionen
â”‚   â”‚   â””â”€â”€ constants.py       # Asset-Definitionen & Konstanten
â”‚   â”œâ”€â”€ env.py                  # Trading Environment mit Reward Shaping
â”‚   â”œâ”€â”€ rewards.py              # Modulare Reward-Komponenten
â”‚   â”œâ”€â”€ patterns.py             # Pattern Detection (FVG, Order Blocks)
â”‚   â”œâ”€â”€ agent.py                # PPO Agent mit Custom Features
â”‚   â””â”€â”€ data_feed.py            # Binance API Integration
â”œâ”€â”€ tests/                      # Tests (zukÃ¼nftig)
â”œâ”€â”€ backup_20250917/            # Backup der alten Struktur
â”œâ”€â”€ models/                     # Gespeicherte Modelle
â”œâ”€â”€ requirements.txt            # Python Dependencies
â”œâ”€â”€ .env.template              # Environment Template
â””â”€â”€ README.md                  # Diese Datei
```

## ğŸ® Wie das Human Feedback funktioniert

### 1. Demo Mode - Sammle Demonstrationen
Genau wie im Trackmania Video, wo der Entwickler manuell driftet:

```python
# Du tradest manuell:
Your action (0=Hold, 1=Buy, 2=Sell): 1
âœ… Trade executed: BUY at $45,230.50

# Du bewertest deinen Trade:
Rate this trade (-1=bad, 0=neutral, 1=good): 1
ğŸ“ Feedback recorded: 1.0
```

### 2. Training Mode - Periodisches Feedback
WÃ¤hrend das AI trainiert, fragst du periodisch:

```python
=== FEEDBACK REQUEST ===
Recent average reward: 12.5
Episode: 50

Recent trades:
  buy at 45230.50
  sell at 45280.20
  buy at 45150.30

Rate recent performance (-1 to 1, or skip): 0.8
```

### 3. Pattern-spezifisches Feedback
Das System erkennt wenn bestimmte Patterns auftreten:

```python
ğŸ¯ Patterns: FVG=True, OB=True
# AI traded in einer FVG Zone bei einem Order Block
# Dein Feedback: +1 (gut!)
# â†’ System lernt: "FVG + Order Block = gute Trading-Gelegenheit"
```

## âš™ï¸ Konfiguration

### Reward Gewichtungen anpassen

```python
# WÃ¤hrend Training oder in Code:
env.set_reward_weight("fvg", 1.5)        # Mehr FVG Fokus
env.set_reward_weight("human", 3.0)      # Human Feedback wichtiger
env.set_reward_weight("risk_management", 0.8)  # Weniger Risk Focus
```

### Environment Parameter

```python
env = InteractiveTradingEnv(
    df=data,
    initial_cash=10000,          # Starting Capital
    transaction_cost=0.001,      # 0.1% Trading Fees
    max_position_size=1.0,       # Max 100% invested
    enable_patterns=True,        # Pattern Detection aktiviert
    reward_config={
        'weights': {
            'pnl': 1.0,
            'fvg': 0.8,
            'human': 2.0
        }
    }
)
```

## ğŸ“ˆ Beispiel Training Session

```bash
# 1. Sammle erst Demonstrationen
python src/main.py --mode demo --episodes 5

# 2. Trainiere mit deinem Feedback
python src/main.py --mode train --timesteps 20000

# 3. Evaluiere Performance
python src/main.py --mode eval --episodes 10

# 4. Weitere Verbesserungen mit mehr Feedback
python src/main.py --mode train --timesteps 10000
```

## ğŸ”´ Live Trading (Paper)

**âš ï¸ WARNUNG: Nur mit gut-trainierten Modellen verwenden!**

```bash
# Setup API Keys in .env
BINANCE_API_KEY=your_key_here
BINANCE_API_SECRET=your_secret_here

# Starte Live Paper Trading
python src/main.py --mode live --live
```

## ğŸ“Š Monitoring & Analysis

### Training Logs
- TensorBoard Logs in `./tensorboard_logs/`
- Wandb Integration optional verfÃ¼gbar
- Human Feedback wird in `.pkl` Dateien gespeichert

### Performance Metriken
- Portfolio Value & PnL
- Sharpe Ratio & Max Drawdown
- Win Rate & Trade Count
- Pattern Hit Rate (FVG, Order Blocks)

## ğŸ¤ Contributing

Dieses System ist designed um erweitert zu werden:

1. **Neue Patterns hinzufÃ¼gen**: Erweitere `patterns.py`
2. **Neue Rewards**: FÃ¼ge Komponenten in `rewards.py` hinzu
3. **Andere Assets**: Modifiziere `data_feed.py` fÃ¼r Forex/Stocks
4. **Different Agents**: Implementiere SAC oder andere RL Algorithms

## ğŸ“š Inspiration

Dieses System ist inspiriert vom [Trackmania AI Video](https://www.youtube.com/watch?v=Ih8EfvOzBOY), wo gezeigt wurde wie Human Feedback verwendet werden kann um einer AI spezifische Verhaltensweisen beizubringen - in dem Fall Driften.

Hier bringen wir der AI bei:
- **FVG Zones** wie Drift-Punkte anzusteuern
- **Order Blocks** wie Checkpoints zu respektieren
- **Risk Management** wie Speed Control zu praktizieren
- **Pattern Recognition** wie Track-Analyse zu betreiben

## âš ï¸ Disclaimers

- **Kein Financial Advice**: Dies ist ein Lern-/Forschungsprojekt
- **Paper Trading**: Verwende nie echtes Geld ohne extensive Tests
- **API Limits**: Respektiere Binance Rate Limits
- **Risk Management**: Implementiere immer Stop-Losses in Live Trading

## ğŸ¯ Roadmap

- [ ] Streamlit Dashboard fÃ¼r Live Monitoring
- [ ] More Pattern Types (Volume Profile, etc.)
- [ ] Portfolio Optimization Features
- [ ] Multi-Asset Trading Support
- [ ] Advanced Risk Management
- [ ] Backtesting Engine mit Walk-Forward Analysis

---

**Happy Trading! ğŸš€ğŸ“ˆ**

*"Teaching AI to trade is like teaching it to drift - it's all about the right feedback at the right moments."*